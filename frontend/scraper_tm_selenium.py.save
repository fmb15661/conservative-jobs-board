import json
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

OUTPUT = "public/jobs_talentmarket.json"
URL = "https://talentmarket.org/jobs/"

def scrape_talent_market():
    print("Launching Chrome...")

    chrome_options = Options()
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_argument("--disable-infobars")
    chrome_options.add_argument("--start-maximized")

    # Force non-headless
    chrome_options.headless = False

    # Critical fix: allow navigation from automation
    chrome_options.add_argument("--remote-allow-origins=*")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    print("Opening Talent Market…")
    driver.get("about:blank")          # First open blank tab (bypass automation block)
    time.sleep(1)
    driver.execute_script(f"window.location.href = '{URL}';")  # THEN force redirect
    time.sleep(5)

    # Scroll to load everything
    last_h = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        new_h = driver.execute_script("return document.body.scrollHeight")
        if new_h == last_h:
            break
        last_h = new_h

    print("Scanning for job cards...")
    cards = driver.find_elements(By.CSS_SELECTOR, ".content-preview-card")
    print(f"Found {len(cards)} job cards")

    jobs = []

    for card in cards:
        try:
            title_el = card.find_element(By.CSS_SELECTOR, "h2 a")
            title = title_el.text.strip()
            url = title_el.get_attribute("href").strip()

            loc_el = card.find_element(By.CSS_SELECTOR, ".location")
            location = loc_el.text.replace("Location:", "").strip()

            jobs.append({
                "title": title,
                "organization": "Talent Market",
                "location": location,
                "url": url,
                "type": "N/A"
            })

        except:
            pass

    driver.quit()

    print(f"Saving {len(jobs)} jobs to {OUTPUT}…")
    with open(OUTPUT, "w") as f:
        json.dump(jobs, f, indent=2)

    print("Done.")

if __name__ == "__main__":
    scrape_talent_market()
